{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A FULLY CUSTOMISABLE NEURAL NETWORK IN PYTHON FROM SCRATCH\n",
    "Posted on March 30, 2018 by Suyog. _[Visit the article on ML Endeavours](https://mlendeavours.wordpress.com/2018/03/30/a-fully-customizable-neural-network-in-python-from-scratch/)_\n",
    "![alt text](./neural_network.png \"A simple feedforward neural network\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    \"\"\"\n",
    "    A network that uses sigmoid activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    " \n",
    "        self.nodes = []\n",
    "        self.layers = {}\n",
    "        self.weights = {}\n",
    "        self.n_classes = 0        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <kbd>add_layer</kbd> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_layer(self, n_nodes, output_layer=False):\n",
    "    \"\"\"\n",
    "    Adds a layer of specified no. of output nodes.\n",
    "    For the output layer, the flag  output_layer must be True.\n",
    "    A network must have an output layer.\n",
    "    \"\"\"\n",
    "\n",
    "    if not output_layer:\n",
    "        self.nodes.append(n_nodes)\n",
    "    else:\n",
    "        self.n_classes = n_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <kbd>sigmoid</kbd> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(self, z):\n",
    "    \"\"\"\n",
    "    Calculates the sigmoid activation function.\n",
    "    \"\"\"\n",
    " \n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <kbd>predict</kbd> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(self, x, to_predict=True, argmax=True, rand_weights=False):\n",
    "    \"\"\"\n",
    "    Performs a pass of forward propagation.\n",
    "    If to_predict is set to True, trained weights are used\n",
    "    and predictions are returned in a single vector\n",
    "    with labels from 0 to (n_classes - 1).\n",
    "    \"\"\"\n",
    " \n",
    "    nodes = self.nodes\n",
    "    layers = {}\n",
    "    weights = {}\n",
    " \n",
    "    # -------------- for input layer\n",
    "    m = x.shape[0]\n",
    "    x = np.append(np.ones(m).reshape(m, 1), x, axis=1)\n",
    "    layers['a%d' % 1] = x\n",
    " \n",
    "    # --------------- for dense layers\n",
    "    for i in range(len(nodes)):\n",
    "        m, n = x.shape\n",
    "        w = np.random.randn(nodes[i], n) if rand_weights else self.weights['w%d' % (i + 1)]\n",
    "        z = x.dot(w.T)\n",
    "        a = np.append(np.ones(m).reshape(m, 1), self.sigmoid(z), axis=1)\n",
    " \n",
    "        if not to_predict:\n",
    "            layers['a%d' % (i + 2)] = a # We start from a2, as a1 is already done.\n",
    "            weights['w%d' % (i + 1)] = w # We start from w1\n",
    "        x = a # to repeat the same procedure for the next layer.\n",
    " \n",
    "    # --------------- for output layer\n",
    "    m, n = x.shape\n",
    "    w = np.random.randn(self.n_classes, n) if rand_weights else self.weights['w%d' % (len(nodes) + 1)]\n",
    "    z = x.dot(w.T)\n",
    "    a = self.sigmoid(z)\n",
    "    output = a\n",
    " \n",
    "    if not to_predict:\n",
    "        layers['a%d' % (len(layers) + 1)] = a\n",
    "        weights['w%d' % (len(weights) + 1)] = w\n",
    " \n",
    "    if to_predict:\n",
    "        return np.argmax(output, axis=1)\n",
    "    elif rand_weights:\n",
    "        self.layers = layers\n",
    "        self.weights = weights\n",
    "    else:\n",
    "        return layers, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <kbd>cost</kbd> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(self, x, y, lamda=0):\n",
    "    \"\"\"\n",
    "    Calculates the cost for given data and labels, with trained weights.\n",
    "    \"\"\"\n",
    " \n",
    "    weights = self.weights\n",
    "    layers, _ = self.predict(x, predict=False)\n",
    " \n",
    "    m, n = x.shape\n",
    "    reg2 = 0 # The regularization term\n",
    "    for i in range(len(weights)):\n",
    "        reg2 += np.sum(weights['w%d' % (i + 1)][:, 1:] ** 2) # L2 regularization\n",
    " \n",
    "    j = (-1 / m) * np.sum(y.T.dot(np.log(layers['a%d' % (len(layers))])) +\n",
    "                          (1 - y).T.dot(np.log(1 - layers['a%d' % (len(layers))]))) + (lamda / (2 * m)) * reg2\n",
    " \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <kbd>fit</kbd> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(self, data, labels, test=[], test_labels=[], alpha=0.01, lamda=0, epochs=50):\n",
    "    \"\"\"\n",
    "    Performs specified no. of epoches.\n",
    "    One epoch = one pass of forward propagation + one pass of backpropagation.\n",
    "    \"\"\"\n",
    "    \n",
    "    self.predict(data, predict=False, rand_weights=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        layers, weights = self.predict(data, predict=False, rand_weights=False)\n",
    "        m, n = data.shape # Needed later in code\n",
    "        \n",
    "        # ----------------- Calculating del terms\n",
    "        delta_n = layers['a%d' % (len(layers))] - labels\n",
    "        delta_n_1 = delta_n.dot(weights['w%d' % (len(weights))]) * layers['a%d' % (len(layers) - 1)] * \\\n",
    "                            (1 - layers['a%d' % (len(layers) - 1)])\n",
    "\n",
    "        dels = {\n",
    "                'del%d' % (len(layers)): delta_n,\n",
    "                'del%d' % (len(layers) - 1): delta_n_1\n",
    "                }\n",
    "        \n",
    "        for i in range(len(weights) - 2):\n",
    "                    delta = delta[:, 1:].dot(weights['w%d' % (len(layers) - 2 - i)]) * \\\n",
    "                            layers['a%d' % (len(layers) - 2 - i)] * (1 - layers['a%d' % (len(layers) - 2 - i)])\n",
    "                    dels['del%d' % (len(layers) - 2 - i)] = delta  \n",
    "                    \n",
    "        # ------------------ Calculating grad and regularization terms\n",
    "        grads = {\n",
    "                'grad%d' % (len(weights)): (1 / m) * (\n",
    "                    dels['del%d' % (len(layers))].T.dot(layers['a%d' % (len(weights))]))\n",
    "            }\n",
    "\n",
    "        regs = {\n",
    "                'reg%d' % (len(weights)): (lamda / m) * weights['w%d' % (len(weights))]\n",
    "            }\n",
    "\n",
    "        for i in range(len(weights) - 1):\n",
    "                grad = (1 / m) * \\\n",
    "                       (dels['del%d' % (len(layers) - 1 - i)][:, 1:].T.dot(layers['a%d' % (len(weights) - 1 - i)]))\n",
    "\n",
    "                grads['grad%d' % (len(weights) - 1 - i)] = grad\n",
    "\n",
    "                reg = (lamda / m) * weights['w%d' % (len(weights) - 1 - i)]\n",
    "                reg[:, 0] = 0\n",
    "                regs['reg%d' % (len(weights) - 1 - i)] = reg\n",
    "\n",
    "        # ----------------- Updating Parameters\n",
    "        for i in range(1, len(weights) + 1):\n",
    "            weights['w%d' % i] = weights['w%d' % i] - alpha * grads['grad%d' % i] - regs['reg%d' % i]\n",
    "\n",
    "        self.layers = layers\n",
    "        self.weights = weights\n",
    "\n",
    "        # ------------------ Progress bar\n",
    "        print(\"\\r\" + \"{}% |\".format(int(100 * i / epochs) + 1) + '#' * int((int(100 * i / epochs) + 1) / 5) +\n",
    "              ' ' * (20 - int((int(100 * i / epochs) + 1) / 5)) + '|',\n",
    "              end=\"\") if not i % (epochs / 100) else print(\"\", end=\"\")\n",
    "\n",
    "        # ------------------ Train accuracy and cost\n",
    "        acc = 100 * np.sum(np.argmax(layers['a%d' % (len(layers))], axis=1) == np.argmax(labels, axis=1)) / m\n",
    "        j_train = float(self.cost(data, labels, lamda=lamda))\n",
    "\n",
    "        # ------------------ Test accuracy and cost (if valid)\n",
    "        if len(test):\n",
    "            m1, n1 = test.shape\n",
    "            j_test = float(self.cost(test, test_labels, lamda=lamda))\n",
    "            test_prediction = self.predict(test)\n",
    "            acc_test = 100 * np.sum(test_prediction == np.argmax(test_labels, axis=1)) / m1\n",
    "            print('Train cost: %0.2f\\tTrain acc.: %0.2f%%\\tTest cost: %0.2f\\tTest acc.: %0.2f%%' % (j, acc, j_test, acc_test))\n",
    "        else:\n",
    "            print('cost: %0.2f\\tacc.: %0.2f%%' % (j, acc))\n",
    "    \n",
    "    # ------------ Final cost and accuracy\n",
    "    if len(test):\n",
    "        print('Final train cost: %0.2f\\tFinal train acc.: %0.2f%%\\tFinal test cost: %0.2f\\tFinal test acc.: %0.2f%%' % (j_train, acc_train, j_test, acc_test))\n",
    "    else:\n",
    "        print('Final cost: %0.2f\\tFinal acc.: %0.2f%%' % (j_train, j_acc))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
